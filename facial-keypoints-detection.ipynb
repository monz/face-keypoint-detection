{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sey4HweSLsfs"
   },
   "source": [
    "# This is a Kaggle challenge\n",
    "\n",
    "see https://www.kaggle.com/c/facial-keypoints-detection/overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6743,
     "status": "ok",
     "timestamp": 1602440926825,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "l-YyIdrQLsfu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# configuration for notebook\n",
    "IN_COLAB = False\n",
    "USE_GPU = IN_COLAB and False  # TPU and GPU only available in COLAB environment\n",
    "USE_TPU = IN_COLAB and ((not USE_GPU) ^ False)  # XOR; either use GPU or TPU, cannot use both at the same time\n",
    "\n",
    "BASE_PATH = '.'\n",
    "LOG_DIR = os.path.join(BASE_PATH, 'logs')\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_PATH, 'face_keypoint_model')\n",
    "KEEP_MODEL = True  # when true, existing model gets loaded, otherwhise new model gets trained\n",
    "\n",
    "# from google.colab import drive\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = '/content/drive'\n",
    "    drive.mount(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6743,
     "status": "ok",
     "timestamp": 1602440926825,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "l-YyIdrQLsfu"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "TRAIN_DATA_FILE = os.path.join(BASE_PATH, 'data', 'training.csv')\n",
    "TEST_DATA_FILE = os.path.join(BASE_PATH, 'data', 'test.csv')\n",
    "\n",
    "# read raw data from csv\n",
    "train_data_raw = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_data_raw = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1602440931135,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "K0qn12g2Lsfz",
    "outputId": "c9d81d49-a7f4-4a3d-ddf4-582f7a4a511c"
   },
   "outputs": [],
   "source": [
    "# print column names\n",
    "print(train_data_raw.columns)\n",
    "print()\n",
    "print(test_data_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1602440936560,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "8JUDHPXsLsf5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# NOT REQUIRED ANYMORE, BECAUSE 'NAN' VALUES GET HANDLED\n",
    "# clean all rows which contain 'NA' cells\n",
    "# this might be handled differently in future to have way more training data\n",
    "# but for now lets go on with 'perfect' data rows\n",
    "# train_data = train_data_raw.dropna()\n",
    "\n",
    "# FOR DEBUG\n",
    "# which columns contain how many 'missing' values\n",
    "# print(train_data_raw.isna().sum())\n",
    "# print basic statistic data for columns with most missing data\n",
    "# print(train_data_raw[['left_eye_inner_corner_x', 'left_eye_inner_corner_y',\n",
    "#             'left_eye_outer_corner_x', 'left_eye_outer_corner_y',\n",
    "#             'right_eye_inner_corner_x', 'right_eye_inner_corner_y',\n",
    "#             'right_eye_outer_corner_x', 'right_eye_outer_corner_y',\n",
    "#             'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y',\n",
    "#             'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y',\n",
    "#             'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y',\n",
    "#             'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y',\n",
    "#             'mouth_left_corner_x', 'mouth_left_corner_y',\n",
    "#             'mouth_right_corner_x', 'mouth_right_corner_y',\n",
    "#             'mouth_center_top_lip_x', 'mouth_center_top_lip_y',\n",
    "#             'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y']].describe())\n",
    "\n",
    "# use 'Image' column as input\n",
    "train_input_raw = train_data_raw[train_data_raw.columns[-1]]\n",
    "\n",
    "# use all remaining columns as features for training\n",
    "train_features_raw = train_data_raw[train_data_raw.columns[:-1]]\n",
    "\n",
    "\n",
    "# fill each 'nan' cell with random.normal value\n",
    "# based on column mean and std\n",
    "train_features = train_features_raw.copy()\n",
    "\n",
    "for column in train_features.columns:\n",
    "    filldata_column = np.random.normal(\n",
    "        train_features[column].mean(),\n",
    "        train_features[column].std(),\n",
    "        train_features[column].isna().sum()\n",
    "    )\n",
    "    \n",
    "    indexes = train_features.index[train_features[column].isna() == True]\n",
    "    \n",
    "    filldata_series = pd.Series(filldata_column, index=indexes)\n",
    "    \n",
    "    # simpler but with unpredictable result could be the line,\n",
    "    # the simpler line is only possible without warning, when copying \n",
    "    # train_features_raw.copy() to train_features, no only setting \"equal\"\n",
    "    train_features[column].fillna(filldata_series, inplace=True)\n",
    "    # this line is fairly complicated due to Pandas warning, SettingWithCopy\n",
    "    # see, https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "    # train_features.iloc[:, train_features.columns.get_loc(column)] = \\\n",
    "    #   train_features.loc[:, (column)].fillna(filldata_series)\n",
    "\n",
    "# convert DataFrame to Numpy array\n",
    "train_features = np.array(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13832,
     "status": "ok",
     "timestamp": 1602440953798,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "sgbCzQuZLsf-"
   },
   "outputs": [],
   "source": [
    "# convert image string to image data, and DataFrame to Numpy array\n",
    "train_input = np.array([np.array(row.split(), dtype=np.uint8) for row in train_input_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "executionInfo": {
     "elapsed": 9578,
     "status": "ok",
     "timestamp": 1602440953809,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "p-P2lgbULsgC",
    "outputId": "c3e9723d-99ff-4a5e-bbcb-ab683c261f76"
   },
   "outputs": [],
   "source": [
    "# show example image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extract image dimensions\n",
    "img_width = img_height = np.sqrt(train_input.shape[1]).astype(np.uint8)\n",
    "\n",
    "print('Image width: {}, height: {}'.format(img_width, img_height))\n",
    "\n",
    "plt.imshow(train_input[0, :].reshape((img_width, img_height)), cmap='gray')\n",
    "\n",
    "# show face keypoints on first input image\n",
    "for x, y in zip(train_features[0,::2], train_features[0,1::2]):\n",
    "    print(x, y)\n",
    "    plt.scatter(x, y, color='r', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 2696,
     "status": "error",
     "timestamp": 1602440960639,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "dcNU2W2RQfIJ",
    "outputId": "25e8d919-67c5-4546-9a30-497e71009ef9"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %tensorflow_version 2.x\n",
    "# --------------------- THIS IS FOR GPU \n",
    "# --- activate in edit -> notebook-settings -> hardware acceleration\n",
    "if USE_GPU:\n",
    "    import tensorflow as tf\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "\n",
    "# --------------------- THIS IS FOR TPU \n",
    "# --- activate in edit -> notebook-settings -> hardware acceleration\n",
    "if USE_TPU:\n",
    "    import tensorflow as tf\n",
    "    print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    except ValueError:\n",
    "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1602440969857,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "jo6wMHDuLsgH",
    "outputId": "303861b6-4363-4174-c270-8d3155b7dd18"
   },
   "outputs": [],
   "source": [
    "# define cnn\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def get_model_lenet(input_width, input_height, input_channel_count, output_size):\n",
    "    # similar but not identical to LeNet-5\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.Input(shape=(input_width, input_height, input_channel_count)),\n",
    "        layers.Conv2D(6, (5, 5), padding='same', activation='relu'),\n",
    "        layers.MaxPool2D((2, 2)),\n",
    "        layers.Conv2D(16, (5, 5), padding='valid', activation='relu'),\n",
    "        layers.MaxPool2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='relu'),\n",
    "        layers.Dense(84, activation='relu'),\n",
    "        layers.Dense(output_size)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(0.001),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model(input_width, input_height, input_channel_count, output_size):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.Input(shape=(input_width, input_height, input_channel_count)),\n",
    "        layers.Conv2D(32, (9, 9), padding='same', activation='relu'),\n",
    "        layers.MaxPool2D((2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), padding='valid', activation='relu'),\n",
    "        layers.Conv2D(128, (5, 5), padding='valid', activation='relu'),\n",
    "        layers.Conv2D(192, (3, 3), padding='valid', activation='relu'),\n",
    "        layers.MaxPool2D((2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(output_size),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(0.001),\n",
    "        metrics=['mae', tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# configure cnn model\n",
    "INPUT_CHANNEL_COUNT = 1  # due to gray-scale image we have only one color channel\n",
    "OUTPUT_SIZE = len(train_features_raw.columns)\n",
    "\n",
    "model = get_model(img_width, img_height, INPUT_CHANNEL_COUNT, OUTPUT_SIZE)\n",
    "\n",
    "if USE_TPU:\n",
    "    # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "    with tpu_strategy.scope():\n",
    "        model = get_model(img_width, img_height, INPUT_CHANNEL_COUNT, OUTPUT_SIZE)\n",
    "\n",
    "# print model summary, e.g. layer information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcWDSQJDLsgN"
   },
   "source": [
    "# Network Architecture Search\n",
    "\n",
    "1. dense layers (20-epochs)\n",
    "  - 512 x 512 ok-ish, val-loss unstable very-high to same order of magnitude than loss\n",
    "  - 1024 x 1024 worked good, val-loss seems stable\n",
    "    - dropout on first and second dense; result worsens\n",
    "    - dropout on first dense; better than dropouts on both dense layers but worse than without dropout\n",
    "      - loss is worse but overfitting might be less (loss and val-loss nearly equal)\n",
    "    - weight regularization (L2(0.0001)) on both dense layers; works ok, somehow high val-loss\n",
    "    - weight regularization (L2(0.0001)) on first dense layer; \n",
    "  - 2048 x 1024 val-loss seems unstable very-high to low\n",
    "  - 1024\n",
    "    - loss ok but val-loss not really bad but not good either\n",
    "    - weight regularization (L2(0.0001)) on dense layer before output;  loss ok but val-loss not really reduced\n",
    "  - 512\n",
    "    - nearly same performance as with 1024 neurons in dense layer before output, val-loss seems a little jumpy (low-high)\n",
    "  - 256\n",
    "    - equal performance compared to 512 neurons in dense layer before output\n",
    "  - 128\n",
    "    - performance comparable to 256 neurons, but val-loss a little bit higher\n",
    "  - 64\n",
    "    - this wirks pretty good, equally to 256 neurons before output\n",
    "    - weight regularization (L2(0.0001)) does not help reduce val-loss\n",
    "    - dropout, pretty worsens the loss value\n",
    "\n",
    "2. batch normalization\n",
    "  - worked ok-ish\n",
    "\n",
    "3. network regularization\n",
    "\n",
    "4. dropout?!\n",
    "  - result worsens a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 315392,
     "status": "ok",
     "timestamp": 1602249228264,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "hyC8N-ELLsgS",
    "outputId": "98df5918-049c-4141-a55b-a279aeedd7e3"
   },
   "outputs": [],
   "source": [
    "# load pre-trained model or train new model\n",
    "if os.path.exists(MODEL_OUTPUT_DIR) and KEEP_MODEL:\n",
    "    model = keras.models.load_model(MODEL_OUTPUT_DIR)\n",
    "else:\n",
    "    # get new model, instead of updating existing one\n",
    "    model = None\n",
    "    model = get_model(img_width, img_height, INPUT_CHANNEL_COUNT, OUTPUT_SIZE)\n",
    "    \n",
    "    tfb_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "    \n",
    "    # data normalization\n",
    "    # image values from [0,255] -> [0,1]\n",
    "    train_input_scaled = train_input / 255.0\n",
    "    \n",
    "    # reshape data to match cnn input shape\n",
    "    ds_train_input = train_input_scaled.reshape((-1, img_width, img_height, INPUT_CHANNEL_COUNT))\n",
    "\n",
    "    # todo: scale train_features (x, y coordinates to [0,1])\n",
    "    # train\n",
    "    model.fit(\n",
    "        ds_train_input,\n",
    "        train_features,\n",
    "        epochs=40,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[tfb_callback]\n",
    "    )\n",
    "    \n",
    "    # save the model\n",
    "    os.makedirs(MODEL_OUTPUT_DIR)\n",
    "    model.save(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 3976,
     "status": "ok",
     "timestamp": 1602440989722,
     "user": {
      "displayName": "Markus Monz",
      "photoUrl": "",
      "userId": "06660936981153335738"
     },
     "user_tz": -120
    },
    "id": "-9Gonm5TOqfW",
    "outputId": "2dbff142-6986-4906-9374-9daa8413b167"
   },
   "outputs": [],
   "source": [
    "# load test data (done in first jupyter-notebook cell)\n",
    "# show columns of test data\n",
    "print(test_data_raw.columns)\n",
    "\n",
    "# prepare test data for prediction\n",
    "# convert image string to Numpy array\n",
    "test_image_data = np.array([np.array(row.split(), dtype=np.uint8) for row in test_data_raw['Image']])\n",
    "test_image_ids = np.array(test_data_raw['ImageId']).reshape(-1,1)\n",
    "\n",
    "test_data = np.concatenate((test_image_ids, test_image_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "aC4B4XgRHD3s",
    "outputId": "11c6f696-fc6b-4ac8-f76b-47c10c7e21ad"
   },
   "outputs": [],
   "source": [
    "# predict for some images from test dataset\n",
    "predictions = {}\n",
    "for id, input in zip(test_data[0:5,0], test_data[0:5,1:]):\n",
    "    normalized_input = (input / 255.0).reshape((-1, img_width, img_height, INPUT_CHANNEL_COUNT))\n",
    "    predictions.update({id: model.predict(normalized_input)})\n",
    "\n",
    "\n",
    "# show/plot face keypoint prediction\n",
    "points = predictions[1]\n",
    "\n",
    "plt.imshow(test_data[0, 1:].reshape((img_width, img_height)), cmap='gray')\n",
    "\n",
    "# show face keypoints on first input image\n",
    "for x, y in zip(points[0,::2], points[0,1::2]):\n",
    "    print(x, y)\n",
    "    plt.scatter(x, y, color='r', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare pre-trained face detector\n",
    "import cv2\n",
    "\n",
    "# load pre-trained opencv face detector\n",
    "face_cascade_name = \"/usr/share/opencv4/haarcascades/haarcascade_frontalface_alt.xml\"\n",
    "face_cascade = cv2.CascadeClassifier()\n",
    "\n",
    "# Load the cascades\n",
    "if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):\n",
    "    print('Error loading face cascade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define methods for face key point detection in streaming data, e.g. webcam\n",
    "\n",
    "import skimage.transform as transform\n",
    "\n",
    "def detect_faces(frame):\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # frame_gray = cv.equalizeHist(frame_gray)  # works slightly better without equalizeHist, read documentation for what it is good for, comes from a tutorial\n",
    "    \n",
    "    # detect faces\n",
    "    faces = face_cascade.detectMultiScale(frame_gray)\n",
    "    return faces\n",
    "\n",
    "def mark_faces_in_frame(frame, faces):\n",
    "    for (x, y, w, h) in faces:\n",
    "        frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "    return frame\n",
    "\n",
    "def detect_face_keypoints(frame, faces, model, model_input_size=(96, 96)):\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    predictions = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        # get face patch; region of interest\n",
    "        face_roi = frame_gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # resize ROI to model input size\n",
    "        face = transform.resize(\n",
    "            face_roi,\n",
    "            model_input_size,\n",
    "            order=0,\n",
    "            preserve_range=True).astype(np.uint8)\n",
    "        \n",
    "        # normalize input, add channel dimension\n",
    "        normalized_face = (face / 255.0).reshape(\n",
    "            (-1, model_input_size[0], model_input_size[1], INPUT_CHANNEL_COUNT))\n",
    "        \n",
    "        # predict face key points\n",
    "        predictions.append(model.predict(normalized_face))\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def mark_face_key_points_in_frame(frame, faces, face_key_points, model_inp_width=96, model_inp_height=96):\n",
    "    radius = 2\n",
    "    color = (0, 0, 255)  # BGR; red\n",
    "    border_thickness = -1  # -1 will fill the circle with given color\n",
    "    \n",
    "    for idx, points in enumerate(face_key_points):\n",
    "        for (x, y) in zip(points[0, ::2], points[0, 1::2]):\n",
    "            face_x = faces[idx][0]\n",
    "            face_y = faces[idx][1]\n",
    "            face_w = faces[idx][2]\n",
    "            face_h = faces[idx][3]\n",
    "\n",
    "            frame = cv2.circle(\n",
    "                frame,\n",
    "                (np.int(face_x + (x*(face_w/model_inp_width))), np.int(face_y + (y*(face_h/model_inp_height)))),\n",
    "                radius, color, border_thickness)\n",
    "                \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try face key point prediction on webcam stream data\n",
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "def handle_video_stream(vc):\n",
    "    if vc.isOpened(): # try to get the first frame\n",
    "        could_read_frame, frame = vc.read()\n",
    "    else:\n",
    "        could_read_frame = False\n",
    "\n",
    "    while could_read_frame:\n",
    "        # show frame\n",
    "        cv2.imshow(\"preview\", frame)\n",
    "\n",
    "        # read frame\n",
    "        could_read_frame, frame = vc.read()\n",
    "\n",
    "        # detec faces in frame\n",
    "        faces = detect_faces(frame)\n",
    "\n",
    "        # plot box around faces\n",
    "        frame = mark_faces_in_frame(frame, faces)\n",
    "\n",
    "        # predict face keypoints on webcam live stream data\n",
    "        face_key_points = detect_face_keypoints(frame, faces, model)\n",
    "\n",
    "        # mark face key points in frame\n",
    "        mark_face_key_points_in_frame(frame, faces, face_key_points)\n",
    "\n",
    "        # read key for exit\n",
    "        key = cv2.waitKey(50)\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "\n",
    "try:\n",
    "    handle_video_stream(vc)\n",
    "finally:\n",
    "    vc.release()\n",
    "    cv2.destroyWindow(\"preview\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "facial-keypoints-detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
